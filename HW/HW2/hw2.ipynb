{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03rT--qmugrq"
   },
   "source": [
    "# Assignment #2\n",
    "\n",
    "## Overview \n",
    "* In this assignment, we will perform extractive Question Answering (QA) where we find out <b>start and end position of answer span</b> from a context (short paragraph of wikipedia article) for a question. \n",
    "\n",
    "* For an example, for the question <b>\"What causes precipitation to fall?\"</b> and context <b>\"In meteorology, precipitation is any product\n",
    "of the condensation of atmospheric water vapor\n",
    "that falls under $\\color{red}{\\text{gravity}}$\"</b>, a model should output the start position and end position of <b>gravity</b>, which are both 17 in terms of whitespace tokenization.\n",
    "\n",
    "* You will implement 1) <b>LSTM</b>, 2) <b>Transformer</b> based QA model that output start and end position of the answer span from the input question and context. More details are on the code blocks below.\n",
    "\n",
    "* We have already provided the codes for all the pipelines including preprocessing, training loop and evaluation. All you need to do is implement the three models by filling the blank in each model class we sepcified and train the models with SQuAD dataset.\n",
    "\n",
    "* Report F1 and Exact Match (EM) score for each model.\n",
    "\n",
    "## Reference\n",
    "- Dataset from [SQuAD](https://www.aclweb.org/anthology/D16-1264.pdf).\n",
    "- Pretrained model from [BERT](https://arxiv.org/abs/1908.08962).\n",
    "- Codes from [transformers](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaPHcaxjyXql"
   },
   "source": [
    "## Install libraries - transformers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "28AZQdo_tRQi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/donggyu/anaconda3/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: transformers in /home/donggyu/anaconda3/lib/python3.8/site-packages (4.6.1)\n",
      "Requirement already satisfied: packaging in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (20.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (2.24.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (0.0.10)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: xxhash in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: dill in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: fsspec in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (0.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in /home/donggyu/anaconda3/lib/python3.8/site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: sacremoses in /home/donggyu/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: filelock in /home/donggyu/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: six in /home/donggyu/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: typing-extensions in /home/donggyu/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/donggyu/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: click in /home/donggyu/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/donggyu/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOYy4u-fFgYh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QjMq3KMmyfzN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import transformers\n",
    "from transformers import (AutoTokenizer, \n",
    "                          default_data_collator, \n",
    "                          AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertPreTrainedModel,\n",
    "                          BertModel,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a6BrOQT0cOE"
   },
   "source": [
    "We will use the BERT-mini model (https://arxiv.org/abs/1908.08962) for faster training and less GPU memory in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jvcPvrqP0bjF"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/bert_uncased_L-4_H-256_A-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hguQDd8g3QvQ"
   },
   "source": [
    "Hyperparameters (**Do not modify**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AeJiW3yc3WP3"
   },
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed\n",
    "batch_size = 12\n",
    "device = \"cuda\" # We will use gpu device provided by colab. Please set runtime to use the gpu accelerator in colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFZHkOvOzGpR"
   },
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In below cells, dataset is automatically loaded and preprocessed.\n",
    "**Please don't modify them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1guhvubjzJDx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/donggyu/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "# Load SQuAD v1.1 dataset using datasets library\n",
    "datasets = load_dataset(\"squad\")\n",
    "\n",
    "# Loading Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LjTP6vfj33er"
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Sj2peyF336GA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/donggyu/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16/cache-8fba95718b11e8c9.arrow\n",
      "Loading cached processed dataset at /home/donggyu/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16/cache-77c29ec9128f3494.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3eLXdNzHB0fH"
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          collate_fn=default_data_collator,\n",
    "                          shuffle=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWcbmjVb0NkU"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SelEstW15KgP"
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader) * 2) # Fixed 20210613\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            batch = {k:v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            token_type_ids=batch[\"token_type_ids\"],\n",
    "                            start_positions=batch[\"start_positions\"],\n",
    "                            end_positions=batch[\"end_positions\"])\n",
    "\n",
    "            loss = outputs[0]\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 1000 == 0:\n",
    "                print (f\"Loss:{loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyle7fmQ0VCo"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BCiqwpBPBDsl"
   },
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H8eEUA-mCqrr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/donggyu/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16/cache-3803565c4feecc3f.arrow\n"
     ]
    }
   ],
   "source": [
    "valid_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ooI2pa1vHldx"
   },
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(valid_features.remove_columns(\"offset_mapping\"), \n",
    "                          batch_size=batch_size, \n",
    "                          collate_fn=default_data_collator,\n",
    "                          shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cT4rG-A5D5qZ"
   },
   "outputs": [],
   "source": [
    "# dev_dataset for extracting real answer\n",
    "def evaluate(model):\n",
    "    model.to(device)\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(valid_loader):\n",
    "        batch = {k:v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            token_type_ids=batch[\"token_type_ids\"],)\n",
    "        start_logits, end_logits = outputs[0], outputs[1]\n",
    "        for start_logit, end_logit in zip(start_logits, end_logits):\n",
    "            all_start_logits.append(start_logit.cpu().numpy())\n",
    "            all_end_logits.append(end_logit.cpu().numpy())\n",
    "    return (all_start_logits, all_end_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPdciFWS0aEs"
   },
   "source": [
    "# Postprocess answers for final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4PASTxHvLxWT"
   },
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "n3NbKNBKNpTB"
   },
   "outputs": [],
   "source": [
    "def calculate_score(raw_predictions):\n",
    "    final_predictions = postprocess_qa_predictions(datasets[\"validation\"], valid_features, raw_predictions)\n",
    "    metric = load_metric(\"squad\")\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "    results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMAZhgpN5MwY"
   },
   "source": [
    "##  Model Specification\n",
    "- You need to implement LSTM and Transformer based QA models.\n",
    "- Following BERT, we concatenate an input question and context. For instance,\n",
    "$$(\\text{[CLS ]}, q_1, \\ldots, q_n, \\text{[SEP]}, c_1, \\ldots, c_m, \\text{[SEP]} )$$ where $q_t, c_{t^\\prime}$ denote a token of question and context respectively and [CLS], [SEP] are special tokens. \n",
    "- dimension of word embedding: 30522\n",
    "\n",
    "## Arguments of forward function\n",
    "Followings are the arguments of forward function:\n",
    "\n",
    "- input_ids: Two dimensional  long Tensor indices of concatenated input question and context tokens ($q_t$ or $c_{t^\\prime}$). Each token is mapped to the unique number (index).\n",
    "\n",
    "- token_type_ids: Two dimensional long Tensor consisting of 0 or 1 with the same size of the input_ids. It indicates whether each token belongs to the question or context. \n",
    "\n",
    "- attention_mask: Two dimensional long Tensor consisting of 0 or 1 with the same size of the input_ids. Since we are dealing with variable length of sequences (the length of each sequence is different), we add zero-padding token to construct fixed size tensor. 0 indicates the zero-padding token which the model should not attend and 1 indicates the the other tokens.  \n",
    "\n",
    "- start_positions: One dimensional long Tensor which indicates the start position of each example.\n",
    "\n",
    "- end_positions: One dimensional long Tensor which indicates the end position of each example.\n",
    "\n",
    "## Specification of forward function\n",
    "- In the forward function, you will get iput_ids, token_type_ids, attention_mask, start_positions, end_positions as arguments. start_positions and end_postions are not given for the test time. You just set them None as default value.\n",
    "\n",
    "- You need to return loss, start_logits, and end_logits when the start_positions and end_positions are given. The start_logits and end_logits denote the unnormalized score (before softmax) for start and end positions.\n",
    "\n",
    "- If the start_positions and end_positions are not given, you return start_logits and end_logits.\n",
    "\n",
    "- Check out the forward function of BERTModel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UGlEGmH1QHf"
   },
   "source": [
    "# **Problem 0. Sample Pipeline using pre-trained BERT model**\n",
    "\n",
    "Please run the below sample pipeline and report exact match / f1 score from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DclCkYRzHd4y"
   },
   "outputs": [],
   "source": [
    "# BERT model (from pre-trained checkpoint)\n",
    "class BERTModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        output = (start_logits, end_logits)\n",
    "        return ((total_loss,) + output) if total_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sXAqqdgKEJ5p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BERTModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERTModel were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036182c4b3ce4c02b69584f69f55701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2.9935789108276367\n",
      "Loss:3.166799545288086\n",
      "Loss:1.9664857387542725\n",
      "Loss:2.601724624633789\n",
      "Loss:2.7610039710998535\n",
      "Loss:1.7573943138122559\n",
      "Loss:1.1449921131134033\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e300d29ad34cad8ad015c89af6c1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.8035937547683716\n",
      "Loss:1.4644100666046143\n",
      "Loss:2.5865137577056885\n",
      "Loss:1.9616796970367432\n",
      "Loss:1.6259715557098389\n",
      "Loss:1.6531521081924438\n",
      "Loss:1.909135341644287\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da30002828be4ac4ae630390dc6be8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=899.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fbe0a4369b4fe0ba14728a384b28ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 60.0, 'f1': 70.67257316462724}\n"
     ]
    }
   ],
   "source": [
    "model = BERTModel.from_pretrained(model_checkpoint)\n",
    "train(model)\n",
    "raw_predictions = evaluate(model)\n",
    "calculate_score(raw_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Axj3lAqhiGL"
   },
   "source": [
    "# **Problem 1. LSTM Implementation**\n",
    "\n",
    "We provide the skeleton code for LSTM QA model.\n",
    "Fill the init and forward function following descriptions in pdf file.\n",
    "\n",
    "Then, run the pipeline and report the exact match / f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wFhzJSAI5PoA"
   },
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(30522, dim)\n",
    "\n",
    "        ### Implement the model here ###\n",
    "        self.lstm=torch.nn.LSTM(input_size=dim,hidden_size=dim)\n",
    "        self.qa_outputs=nn.Linear(dim,2)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask, \n",
    "        token_type_ids, \n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "    ):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        \n",
    "        ## Implement the LSTM forward and qa_outputs (linear) forward here ###\n",
    "        pack=torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embeds,\n",
    "            lengths=torch.sum(attention_mask,dim=1).tolist(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        outputs=self.lstm(pack)\n",
    "        sequence_output,_=torch.nn.utils.rnn.pad_packed_sequence(outputs[0],batch_first=True)\n",
    "        logits=self.qa_outputs(sequence_output)\n",
    "        ####\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        output = (start_logits, end_logits)\n",
    "        return ((total_loss,) + output) if total_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HZ3D07mEFuzq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eeb58c78b3472b91fff7c24bc5e1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4.193328857421875\n",
      "Loss:4.639159202575684\n",
      "Loss:4.669092655181885\n",
      "Loss:3.994865894317627\n",
      "Loss:4.607333183288574\n",
      "Loss:4.134253978729248\n",
      "Loss:4.328581809997559\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8be76c692014de08c17efaefbaec43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4.036929130554199\n",
      "Loss:3.9858651161193848\n",
      "Loss:4.279071807861328\n",
      "Loss:4.3112382888793945\n",
      "Loss:3.746023654937744\n",
      "Loss:3.747622013092041\n",
      "Loss:3.971881866455078\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bd7c429cd443569d0f49691af4cb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=899.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d733a5043383400b8289e1dbfdfed5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 5.089877010406812, 'f1': 11.49986205468953}\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 768\n",
    "\n",
    "model = LSTMModel(hidden_dim)\n",
    "train(model)\n",
    "raw_predictions = evaluate(model)\n",
    "calculate_score(raw_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-ZBFxHUh_U-"
   },
   "source": [
    "# **Problem 2. Transformer Implementation**\n",
    "\n",
    "We provide the skeleton code for Transformer QA model.\n",
    "Fill the init and forward function following descriptions in pdf file.\n",
    "\n",
    "Then, run the pipeline and report the exact match / f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5wnv2u1P5XCG"
   },
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding = nn.Embedding(30522, dim)\n",
    "\n",
    "        ### Implement the model here ###\n",
    "        self.encoder=torch.nn.TransformerEncoderLayer(d_model=dim,nhead=num_heads)\n",
    "        self.qa_outputs=nn.Linear(dim,2)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask, \n",
    "        token_type_ids, \n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "    ):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        b,t = input_ids.size()\n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(-1, self.num_heads, t, -1).reshape(b * self.num_heads, t, -1)\n",
    "        attention_mask = attention_mask == 0\n",
    "        ## Implement the Transformer forward and qa_outputs (linear) forward here ###\n",
    "        outputs=self.encoder(\n",
    "            src=torch.transpose(embeds,0,1),\n",
    "            src_mask=attention_mask\n",
    "        )\n",
    "        sequence_output=torch.transpose(outputs,0,1)\n",
    "        logits=self.qa_outputs(sequence_output)\n",
    "        ####\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        output = (start_logits, end_logits)\n",
    "        return ((total_loss,) + output) if total_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5Y72Ihy0P4vY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3572a4e792dd4e66a19ae04e1e330288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4.756199836730957\n",
      "Loss:4.391205310821533\n",
      "Loss:4.321883201599121\n",
      "Loss:4.261214733123779\n",
      "Loss:4.342556476593018\n",
      "Loss:4.480454444885254\n",
      "Loss:3.9709627628326416\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19edb12c34483481f8184a875717dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4.533720016479492\n",
      "Loss:4.415839672088623\n",
      "Loss:4.716879367828369\n",
      "Loss:4.258673667907715\n",
      "Loss:3.997011423110962\n",
      "Loss:4.100652694702148\n",
      "Loss:4.146346092224121\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaac4acb81a4664a32b42f91e8e095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=899.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad6ab9d4ee74d6d896cd35c33fec555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 5.089877010406812, 'f1': 10.593915244581673}\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 768\n",
    "head_size = 12\n",
    "\n",
    "model = TransformerModel(hidden_dim, head_size)\n",
    "train(model)\n",
    "raw_predictions = evaluate(model)\n",
    "calculate_score(raw_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0LjcMsiFKI"
   },
   "source": [
    "# **Problem 3. Analysis Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCAf1QCziMYL"
   },
   "source": [
    "### 3.1. Which model performed better, LSTM or Transformer? Explain why it performed better.\n",
    "My implementation results for LSTM/Transformer based QA models are as follows:\n",
    "\n",
    "|Model|exact_match|f1|\n",
    "|-----|-----------|--|\n",
    "|LSTM|5.089877010406812|11.49986205468953|\n",
    "|Transformer|5.089877010406812|10.593915244581673|\n",
    "\n",
    "Amazingly, LSTM and Transformer got the exactly same *exact_match* scores. (But it is completly coincidential.)   \n",
    "LSTM got higher f1 score where the number of parameters for each models are in the similar level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters in LSTM: 28167170\n",
      "# of parameters in Transformer: 28956418\n"
     ]
    }
   ],
   "source": [
    "print('# of parameters in LSTM: '+str(sum(p.numel() for p in LSTMModel(hidden_dim).parameters())))\n",
    "print('# of parameters in Transformer: '+str(sum(p.numel() for p in TransformerModel(hidden_dim, head_size).parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though, *exact_macth* is a strict all-or-nothing metric so *f1* score is more practical in common sense.\n",
    "In this respect, LSTM performed better than Transformer.\n",
    "This might be because my models are small. The strength of Transformer is good scalability by its non-sequential structure.\n",
    "However, as the original paper of Transformer proposed *N* encoder/decoder layers, it requires many layers to perform well.\n",
    "Since I used only one encoder layer for Transformer model, LSTM was able to outperform Transformer in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JclkUFK4iw9V"
   },
   "source": [
    "### 3.2. Which model shows faster training, LSTM or Transformer? Explain what makes the training faster.\n",
    "Transformer trained few minutes faster than LSTM.\n",
    "The original paper of Transformer (https://arxiv.org/pdf/1706.03762.pdf) is explaining about this;\n",
    "\"In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
    "length $n$ is smaller than the representation dimensionality $d$ ... \"\n",
    "In more detail, the complexity per layer for self-attention layer is $O(n^2\\cdot d)$, while the one for recurrent layer is $O(n\\cdot d^2)$.\n",
    "In our case, the sequence length $n$ is less or equal to 384, and the representation dimensionality $d$ is 768. Thus, theoritically and empirically, Transformer trains faster than LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2hlbBCwjCCG"
   },
   "source": [
    "### 3.3. Why the pre-trained model performs much better than LSTM and Transformer? Please write down your thoughts.\n",
    "The pre-trained model has preliminary knowledge on NLU. This surely benefits the model to understand feature representations. So the model can train and perform much better than my LSTM and Transformer models that are trained from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XC1Ysu4jQRM"
   },
   "source": [
    "### 3.4. How to improve the performance of the QA model with pre-trained BERT-mini? Please write your ideas if any.\n",
    "Possible attempts to improve the model\n",
    "- Larger batch\n",
    "- Other loss functions instead of cross entropy loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_fix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
